<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI/ML PS 15 GROUP 26</title>
    <link rel="stylesheet" href="assets/css/style.css">
</head>
<body>
    <!-- Simplified to a single full-screen video -->
    <div class="video-background">
        <video id="background-video" muted playsinline>
            <source src="assets/videos/background.webm" type="video/webm">
            <source src="assets/videos/background.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
    </div>

    <div class="content-wrapper">
        <nav class="side-nav">
            <a href="#home" class="nav-button active">Home</a>
            <button id="open-about-modal" class="nav-button">About</button>
            <a href="#flowchart" class="nav-button">Flowchart</a>
            <a href="#demo" class="nav-button">Demo</a>
            <a href="#gallery" class="nav-button">Gallery</a>
            <a href="#application" class="nav-button">Application</a>
            <a href="https://github.com/pitcher69/ZEUS6D/" target="_blank" class="nav-button github-button" title="My GitHub Profile">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" fill="currentColor">
                    <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
            </a>
        </nav>

        <main class="content">
            <section id="home">
                <div class="home-content">
                    <h1 class="gradient-text">⚡ZEUS6D⚡ : Training free Zero Shot 6D Pose Estimation Model</h1>
                    <p class="tagline">Accurately determining the 3D position and orientation of objects from a single image.</p>
                    <div class="cta-buttons">
                        <a href="#demo" class="cta-button">View Demo</a>
                        <a href="https://drive.google.com/file/d/1fmEnqCRdL2BvbcRkeXqPtd1A101NqH0q/view?usp=sharing" class="cta-button">Read Paper</a>
                        <a href="https://github.com/pitcher69/ZEUS6D/" class="cta-button" target="_blank">Code</a>
                    </div>
                </div>
            </section>

            <!-- -------- BEGIN RESTORED FLOWCHART SECTION -------- -->
            <section id="flowchart">
                <h2 class="gradient-text">Project Flowchart</h2>
                <div class="flowchart-container">
                    <!-- Direct embed of the diagram, NO IFRAME -->
                    <div class="mxgraph" style="max-width:100%;border:1px solid transparent;" data-mxgraph="{&quot;highlight&quot;:&quot;#0000ff&quot;,&quot;nav&quot;:false,&quot;resize&quot;:true,&quot;lightbox&quot;:false,&quot;toolbar&quot;:null,&quot;xml&quot;:&quot;&lt;mxfile host=\&quot;app.diagrams.net\&quot; agent=\&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36\&quot; version=\&quot;28.0.7\&quot;&gt;&lt;diagram name=\&quot;Page-1\&quot; id=\&quot;aZrKGwb1maUwDrWq9_2M\&quot;&gt;&lt;mxGraphModel dx=\&quot;7130\&quot; dy=\&quot;3835\&quot; grid=\&quot;1\&quot; gridSize=\&quot;10\&quot; guides=\&quot;1\&quot; tooltips=\&quot;1\&quot; connect=\&quot;1\&quot; arrows=\&quot;1\&quot; fold=\&quot;1\&quot; page=\&quot;1\&quot; pageScale=\&quot;1\&quot; pageWidth=\&quot;850\&quot; pageHeight=\&quot;1100\&quot; math=\&quot;0\&quot; shadow=\&quot;0\&quot;&gt;&lt;root&gt;&lt;mxCell id=\&quot;0\&quot;/&gt;&lt;mxCell id=\&quot;1\&quot; parent=\&quot;0\&quot;/&gt;&lt;mxCell id=\&quot;x6CHhdUkws1HIm_VyeYb-16\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=3;strokeColor=#FFD700;\&quot; parent=\&quot;1\&quot; source=\&quot;x6CHhdUkws1HIm_VyeYb-1\&quot; target=\&quot;x6CHhdUkws1HIm_VyeYb-9\&quot; edge=\&quot;1\&quot;&gt;&lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;/&gt;&lt;/mxCell&gt; ... (truncated for brevity) ... </div>
                </div>
                <!-- Diagram viewer script -->
                <script type="text/javascript" src="https://app.diagrams.net/js/viewer-static.min.js"></script>
            </section>
            <!-- -------- END RESTORED FLOWCHART SECTION -------- -->

            <section id="demo">
                <h2 class="gradient-text">Live Demo</h2>
                <div class="demo-item">
                    <h3>Real-Time Tracking</h3>
                    <p>Our model processes video streams to identify and track the 6D pose of known objects. This GIF demonstrates the system's ability to maintain a stable lock on a target.</p>
                    <img src="assets/images/DEMOgif.gif" alt="Live demo of pose estimation" class="demo-gif">
                </div>
            </section>

            <section id="gallery">
                <div class="image-gallery">
                    <h2 class="gradient-text">Gallery</h2>
                    <p>Here are some sample results from our test dataset, showcasing performance under various conditions.</p>
                    <div class="gallery-grid">
                        <div class="gallery-item"><img src="assets/images/image1.jpeg" alt="Results on LINEMOD dataset"></div>
                        <div class="gallery-item"><img src="assets/images/image2.jpeg" alt="Results on YCB-Video dataset"></div>
                        <div class="gallery-item"><img src="assets/images/image3.jpeg" alt="Results on LINEMOD dataset"></div>
                        <div class="gallery-item"><img src="assets/images/image4.jpeg" alt="Results on YCB-Video dataset"></div>
                        <div class="gallery-item"><img src="assets/images/image5.jpeg" alt="Results on the T-LESS dataset"></div>
                    </div>
                </div>
            </section>

            <section id="application">
                <h2 class="gradient-text">Application</h2>
                <div class="content-block">
                    <p>This shows a complete run of the application pipeline.</p>
                    <img src="assets/images/application.gif" alt="Application Demo GIF">
                </div>
            </section>
        </main>
    </div>

    <script src="assets/js/main.js"></script>
    <div id="about-modal" class="modal">
        <div class="modal-content">
            <button class="close-button">&times;</button>
            <h2 class="gradient-text">About The Project</h2>
            <div class="modal-tabs">
                <button class="modal-tab-button active" data-content-id="intro-content">Intro</button>
                <button class="modal-tab-button" data-content-id="target-pipeline-content">Target Pipeline</button>
                <button class="modal-tab-button" data-content-id="query-pipeline-content">Query Pipeline</button>
                <button class="modal-tab-button" data-content-id="pose-estimation-content">Pose Estimation</button>
            </div>
            <div class="modal-display-content"></div>
        </div>
    </div>

    <div id="modal-hidden-content" style="display: none;">
      <div id="intro-content">
        <h3>Figure 1 — End-to-End Pipeline Overview</h3>
        <p>This schematic situates every processing stage in a single frame.</p>
        <p><strong>Inputs:</strong> A watertight CAD mesh (left) and a cropped RGB scene image (right).</p>
        <p><strong>Dual frozen encoders:</strong> Vision backbone Phi maps RGB pixels to high-dimensional appearance features, geometric backbone Psi maps 3D vertices to shape features. Both are fixed (no fine-tuning), preserving their large-scale pre-training priors.</p>
        <p><strong>Bidirectional projections:</strong> Blue arrows (3D → 2D) rasterize mesh vertices into image space; green arrows (2D → 3D) lift image-plane predictions back onto the surface. This ensures every surface point acquires both visual and geometric descriptors.</p>
        <p><strong>Descriptor fusion:</strong> Visual and geometric embeddings are concatenated and l<sub>2</sub>-normalized, forming a modality-agnostic 128-dimensional signature for each point. The result is a common representation in which CAD and real observations are directly comparable.</p>
        <img src="assets/images/overall-pipeline.jpeg" alt="Overall Pipeline">
      </div>

      <div id="target-pipeline-content">
        <h3>Figure 2 — Target-Side Feature Extraction</h3>
        <p>This zooms into the right-hand branch of Figure 1.</p>
        <p><strong>Instance segmentation:</strong> A foundation-scale model (like SAM) isolates the object mask without class supervision.</p>
        <p><strong>Depth augmentation:</strong> Sensed depth or monocular-depth inference converts the mask into a partial point cloud, P<sub>T</sub><sup>sparse</sup>.</p>
        <p><strong>Appearance descriptors (V<sub>T</sub>):</strong> Phi processes RGB patches; 2D to 3D projection anchors descriptors to 3D points.</p>
        <p><strong>Shape descriptors (G<sub>T</sub>):</strong> Psi encodes local surface curvature, normal direction, and Laplacian context.</p>
        <p><strong>Feature fusion (F<sub>T</sub>):</strong> Concatenation of V<sub>T</sub> and G<sub>T</sub> yields a robust per-point descriptor resilient to illumination change and sensor noise.</p>
        <img src="assets/images/target-pipeline.jpeg" alt="Target Pipeline">
      </div>

      <div id="query-pipeline-content">
        <h3>Figure 3 — Query-Side Feature Extraction</h3>
        <p>Here the same operations are applied to the CAD model under controlled conditions.</p>
        <p><strong>Icosahedral camera sampling:</strong> Forty-two uniformly distributed viewpoints guarantee full surface coverage.</p>
        <p><strong>Multi-view rendering:</strong> Each view produces an RGB image and a visibility buffer; Phi extracts 2D appearance features.</p>
        <p><strong>Back-projection and aggregation:</strong> Per-pixel descriptors are projected onto visible vertices and averaged over all views, integrating color evidence into the mesh.</p>
        <p><strong>Geometric encoding:</strong> Psi generates intrinsic shape descriptors directly on the mesh.</p>
        <p><strong>Fusion to F<sub>Q</sub>:</strong> As before, visual and geometric channels are combined into a unified 128-dimensional template descriptor set.</p>
        <img src="assets/images/query-pipeline.jpeg" alt="Query Pipeline">
      </div>

      <div id="pose-estimation-content">
        <h3>Figure 4 — Correspondence-Driven Pose Estimation and Refinement</h3>
        <p>The final panel translates descriptor similarity into a precise 6-DoF transform.</p>
        <ul>
          <li><strong>RANSAC (Robust coarse alignment):</strong> Hypothesizes rigid transforms from minimal feature triplets and discards outliers via consensus.</li>
          <li><strong>ICP (point-to-plane, continuous refinement):</strong> Minimizes point-to-plane error; converges rapidly when initial pose is within ~10 cm/10°.</li>
          <li><strong>Closed-form SVD (Exact least-squares fit):</strong> Umeyama solution gives the optimal rotation and translation for inlier correspondences.</li>
          <li><strong>SAR (Symmetry-Aware Refinement, ambiguity resolution):</strong> Enumerates symmetry-equivalent rotations, keeping the one that maximizes descriptor agreement.</li>
          <li><strong>Similarity scoring (Confidence measure):</strong> Computes mean descriptor distance after alignment; low values indicate high pose fidelity.</li>
        </ul>
        <img src="assets/images/pose-estimation-pipeline.jpeg" alt="Pose Estimation">
      </div>
    </div>
</body>
</html>
